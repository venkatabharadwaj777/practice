{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'parse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-cc3a41657219>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChromeOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'parse'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "import parse\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(\"./chromedriver\")\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=[\"role\",\"Company\",\"Salary\",\"Location\",\"Experience\",\"Posted\"])\n",
    "\n",
    "for i in range(0,500,10):\n",
    "    driver.get('https://in.indeed.com/jobs?q=web+scraper&l=&start='+str(i))\n",
    "    jobs = []\n",
    "    driver.implicitly_wait(4)\n",
    "    \n",
    "    \n",
    "    for job in driver.find_elements_by_class_name('result'):\n",
    "        \n",
    "        soup = BeautifulSoup(job.get_attribute('innerHTML'),'html.parser')\n",
    "        \n",
    "        try:\n",
    "            role = soup.find(\"a\",class_=\"role\").text.replace(\"\\n\",\"\").strip()\n",
    "\n",
    "        except:\n",
    "            role = 'None'\n",
    "\n",
    "        try:\n",
    "            Company = soup.find(class_=\"Company\").text\n",
    "        except:\n",
    "            Company = 'None'\n",
    "\n",
    "        try:\n",
    "            Salary = soup.find(class_=\"Salary\").text.replace(\"\\n\",\"\").strip()\n",
    "        except:\n",
    "            Salary = 'None'\n",
    "\n",
    "        try:\n",
    "            Location = soup.find(class_=\"Location\").text.replace(\"\\n\",\"\").strip()\n",
    "        except:\n",
    "            Location = 'None'\n",
    "            \n",
    "        try:\n",
    "            Experience=soup.find(class_=\"Experience\").text.replace(\"\\n\",\"\").strip()\n",
    "        except:\n",
    "            Experience='None'\n",
    "\n",
    "        try:\n",
    "            Posted = soup.find(class_=\"Posted\").text\n",
    "        except:\n",
    "            Posted = \"None\"\n",
    "            \n",
    "\n",
    "        df = df.append({'role':role,'Location':Location,\"Company\":Company,\"Salary\":Salary,\"Experience\":Experience,\n",
    "                        \"Posted\":Posted},ignore_index=True)\n",
    "\n",
    "        print(\"Got these many results:\",df.shape)\n",
    "        \n",
    "df.to_excel(r\"C:\\Users\\Venkata Bharadwaj\\Desktop\\datascrap.excel\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in city_set:\n",
    "  for start in range(0, max_results_per_city, 10):\n",
    "  page = requests.get(‘http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=' + str(city) + ‘&start=’ + str(start))\n",
    "  time.sleep(1)  #ensuring at least 1 second between page grabs\n",
    "  soup = BeautifulSoup(page.text, “lxml”, from_encoding=”utf-8\")\n",
    "  for div in soup.find_all(name=”div”, attrs={“class”:”row”}): \n",
    "    #specifying row num for index of job posting in dataframe\n",
    "    num = (len(sample_df) + 1) \n",
    "    #creating an empty list to hold the data for each posting\n",
    "    job_post = [] \n",
    "    #append city name\n",
    "    job_post.append(city) \n",
    "    #grabbing job title\n",
    "    for a in div.find_all(name=”a”, attrs={“data-tn-element”:”jobTitle”}):\n",
    "      job_post.append(a[“title”]) \n",
    "    #grabbing company name\n",
    "    company = div.find_all(name=”span”, attrs={“class”:”company”}) \n",
    "    if len(company) > 0: \n",
    "      for b in company:\n",
    "        job_post.append(b.text.strip()) \n",
    "    else: \n",
    "      sec_try = div.find_all(name=”span”, attrs={“class”:”result-link-source”})\n",
    "      for span in sec_try:\n",
    "        job_post.append(span.text) \n",
    "    #grabbing location name\n",
    "    c = div.findAll(‘span’, attrs={‘class’: ‘location’}) \n",
    "    for span in c: \n",
    "      job_post.append(span.text) \n",
    "    #grabbing summary text\n",
    "    d = div.findAll(‘span’, attrs={‘class’: ‘summary’}) \n",
    "      for span in d:\n",
    "        job_post.append(span.text.strip()) \n",
    "    #grabbing salary\n",
    "    try:\n",
    "      job_post.append(div.find(‘nobr’).text) \n",
    "    except:\n",
    "      try:\n",
    "        div_two = div.find(name=”div”, attrs={“class”:”sjcl”}) \n",
    "        div_three = div_two.find(“div”) \n",
    "        job_post.append(div_three.text.strip())\n",
    "      except:\n",
    "        job_post.append(“Nothing_found”) \n",
    "    #appending list of job post info to dataframe at index num\n",
    "    sample_df.loc[num] = job_post\n",
    "\n",
    "#saving sample_df as a local csv file — define your own local path to save contents \n",
    "sample_df.to_csv(“[filepath].csv”, encoding=’utf-8')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'C:\\Users\\Venkata' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "pip install parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: html.parser. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c20509a3a8d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprettify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[0mbuilder_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuilder_registry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuilder_class\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m                 raise FeatureNotFound(\n\u001b[0m\u001b[0;32m    244\u001b[0m                     \u001b[1;34m\"Couldn't find a tree builder with the features you \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m                     \u001b[1;34m\"requested: %s. Do you need to install a parser library?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: html.parser. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "url='https://in.indeed.com/jobs?q=web%20scraper&l&vjk=23c0844c7b9fbad5'\n",
    "page=requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "print(soup.prettify(encoding=None))\n",
    "\n",
    "def extract_job_title_from_result(soup): \n",
    "    jobs = []\n",
    "    for div in soup.find_all(name=\"div\", attrs={\"class\":\"gnav\"}):\n",
    "        for a in div.find_all(name=\"a\", attrs={\"data-tn-element\":\"vjs-jobTitle\"}):\n",
    "            jobs.append(a[\"title\"])\n",
    "            return(jobs)\n",
    "extract_job_title_from_result(soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
